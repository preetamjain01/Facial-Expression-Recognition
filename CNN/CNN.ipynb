{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "37v9LhnzXOq_"
   },
   "source": [
    "# Facial Expression Recognition using CNN - Part  1/3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PAj_dOwEXSCx"
   },
   "source": [
    "## Abstract\n",
    "\n",
    "* This project is about Facial Expression Recognition using Convolutional Neural Network, VGG19 CNN and Residual Networks:\n",
    "    * Computer animated agents and robots bring new dimension in human computer interaction which makes it vital as how computers can affect our social life in day-to-day activities. Face to face communication is a real-time process operating at a time scale in the order of milliseconds. The level of uncertainty at this time scale is considerable, making it necessary for humans and machines to rely on sensory rich perceptual primitives rather than slow symbolic inference processes. In this project we are presenting the real time facial expression recognition of seven most basic human expressions: ANGER, DISGUST, FEAR, HAPPY, NEUTRAL, SAD, SURPRISE.\n",
    "    \n",
    "* We implement Convolutional Neural Network in this part\n",
    "\n",
    "\n",
    "### Dataset Introduction\n",
    "\n",
    "* The dataset we have chosen is Facial Expression Recognition fer2013.\n",
    "* The training set consists of 28,709 examples. The public test set consists of 3,589 examples. The final test set, consists of another 3,589 examples.\n",
    "* The data consists of 48x48 pixel grayscale images of faces. \n",
    "* The faces have been automatically registered so that the face is more or less centered and occupies about the same amount of space in each image. \n",
    "* The task is to categorize each face based on the emotion shown in the facial expression in to one of seven categories (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral).\n",
    "* The \"emotion\" column contains a numeric code ranging from 0 to 6, inclusive, for the emotion that is present in the image. The \"pixels\" column contains a string surrounded in quotes for each image. The contents of this string a space-separated pixel values in row major order. Our task is to predict the \"emotion\" column.\n",
    "* This dataset was prepared by Pierre-Luc Carrier and Aaron Courville, as part of an ongoing research project. They have graciously provided the workshop organizers with a preliminary version of their dataset to use for this contest.\n",
    "* Link to the dataset - https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data\n",
    "\n",
    "`Conclusion` - We train CNN Sequential model on certain parameters, resulting in the value of accuracy (which is how our goodness of model is evaluated)  as `57.2%`. And thus helping us in making the comparison to different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f3jhS9cXXY9J"
   },
   "source": [
    "### Part 1 - CNN\n",
    "\n",
    "* In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery.\n",
    "* These are a special class of Multilayer perceptron which are well suited for pattern classification. It is specifically designed to recognize 2D shapes with a high level of invariance, skewing and scaling. They are made up of neurons that have learnable weights and biases. Each neuron receives some input, performs a dot product and optionally follows it with a non-linearity. The whole network still expresses a single differentiable score function: from the raw image pixels on one end to class scores at the other. A simple ConvNet is a sequence of layers, and every layer of a ConvNet transforms one volume of activations to another through a differentiable function. There are three main types of layers to build ConvNet architectures: Convolutional Layer, Pooling Layer, and Fully-Connected Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorFlow:\n",
    "\n",
    "TensorFlow is an open source software library for high performance numerical computation. Its flexible architecture allows easy deployment of computation across a variety of platforms (CPUs, GPUs, TPUs), and from desktops to clusters of servers to mobile and edge devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Vhw1VEDFbOMp",
    "outputId": "b52a8bc9-2e85-4f41-88aa-bc823ec47453"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, BatchNormalization\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#cpu - gpu configuration\n",
    "config = tf.ConfigProto( device_count = {'GPU': 1, 'CPU': 56} ) #Providing GPU\n",
    "tf.reset_default_graph() #This will reset the current loaded graph to avoid issues\n",
    "sess = tf.Session(config=config) \n",
    "keras.backend.set_session(sess)\n",
    "num_classes = 7 #angry, disgust, fear, happy, sad, surprise, neutral\n",
    "batch_size = 256\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* On a typical system, there are multiple computing devices. In TensorFlow, the supported device types are CPU and GPU. They are represented as strings. For example:\n",
    "\n",
    "    * \"/cpu:0\": The CPU of your machine.\n",
    "    * \"/device:GPU:0\": The GPU of your machine, if you have one.\n",
    "    * \"/device:GPU:1\": The second GPU of your machine, etc.\n",
    "If a TensorFlow operation has both CPU and GPU implementations, the GPU devices will be given priority when the operation is assigned to a device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B3GKdTKSbPPK"
   },
   "outputs": [],
   "source": [
    "#read the dataset\n",
    "with open(\"../fer2013.csv\") as f:\n",
    "    content = f.readlines()\n",
    "#Converting lines to array as the lines represent the pixels\n",
    "lines = np.array(content)\n",
    "\n",
    "num_of_instances = lines.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The method readlines() reads until EOF using readline() and returns a list containing the lines.\n",
    "* Then np.array() creates an array of the list containing the lines from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N5y1adembT7S"
   },
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = [], [], [], []\n",
    "\n",
    "#creating training and testing data [4]\n",
    "for i in range(1,num_of_instances):\n",
    "    emotion, img, usage = lines[i].split(\",\")\n",
    "    val = img.split(\" \")\n",
    "            \n",
    "    pixels = np.array(val, 'float32')\n",
    "        \n",
    "    emotion = keras.utils.to_categorical(emotion, 7)\n",
    "    #Creating training and testing set\n",
    "    if 'Training' in usage:\n",
    "        y_train.append(emotion)\n",
    "        x_train.append(pixels)\n",
    "    elif 'PublicTest' in usage:\n",
    "        y_test.append(emotion)\n",
    "        x_test.append(pixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The split() method splits a string into a list by the given separator.\n",
    "* keras.utils.to_categorical() - Converts a class vector (integers) to binary class matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "0QDAjt7RfZbp",
    "outputId": "ad84e9ec-0011-4ab2-fa72-aba80eae51c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28709 train samples\n",
      "3589 test samples\n"
     ]
    }
   ],
   "source": [
    "#data transformation for train and test sets\n",
    "x_train = np.array(x_train, 'float32')\n",
    "y_train = np.array(y_train, 'float32')\n",
    "x_test = np.array(x_test, 'float32')\n",
    "y_test = np.array(y_test, 'float32')\n",
    "\n",
    "x_train /= 255 #normalize inputs between [0, 1]\n",
    "x_test /= 255\n",
    "\n",
    "#Since the images are 48x48, reshaping the array to 48x48\n",
    "x_train = x_train.reshape(x_train.shape[0], 48, 48, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.reshape(x_test.shape[0], 48, 48, 1)\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The np.array() method creates an array of the given array like objects such as x_train, y_train, x_test and y_test into a desired data type 'float32'\n",
    "* .reshape() - Reshapes the images in the array object to 48x48 without changing its data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "id": "Zp8OS0GOTNkI",
    "outputId": "e15e2df2-2f86-4510-af30-b81bec9dce98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "#------------------------------\n",
    "#construct CNN structure\n",
    "model = Sequential()\n",
    "\n",
    "#1st Convolution layer\n",
    "model.add(Conv2D(64, (5, 5), activation='relu', input_shape=(48,48,1)))\n",
    "model.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "\n",
    "#2nd Convolution layer\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "\n",
    "#3rd Convolution layer\n",
    "model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "#Fully Connected Layers\n",
    "\n",
    "model.add(Dense(1024))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1024))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline CNN model\n",
    "\n",
    "We try to train one small CNN with a few layers and few filters per layer on our data, which is a VGG-like convnet and we resize the image to 48x48 in order to train it on our own computer cpu.\n",
    "\n",
    "A convnet takes an image expressed as an array of numbers, applies a series of operations to that array and, at the end, returns the probability that an object in the image belongs to a particular class of objects.\n",
    "\n",
    "This CNN has 3 convolutional layers and uses ReLU(rectified linear units) activation function. And it uses maxpooling layers for downsampling and dropout layers to reduce overfitting by a layer from seeing twice the exact same pattern.\n",
    "\n",
    "We set the parameters of the first convolutional layers in the first block to 64 filters and the kernal size is 5x5 and the filter number in convolutional layers of the second block is 128 with the size 3x3 and in third block is 256 with the size of 3x3 in order to fit our dataset.\n",
    "\n",
    "We use Relu, the function f(x) = max(0, x), as our nonlinearity layer because it makes the network train faster and can reduce vanishing gradient problem, which is the issue where the lower layers of the network train very slowly because the gradient decreases exponentially through the layers.\n",
    "\n",
    "And also dropout layer “drops out” a random set of activations in that layer by setting them to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3464
    },
    "colab_type": "code",
    "id": "WfYgNG-ITNkL",
    "outputId": "6061662f-56d8-4520-adf0-ddb6a86f4eb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "256/256 [==============================] - 28s 110ms/step - loss: 1.6280 - acc: 0.3928\n",
      "Epoch 2/100\n",
      "256/256 [==============================] - 24s 93ms/step - loss: 1.2104 - acc: 0.5527\n",
      "Epoch 3/100\n",
      "256/256 [==============================] - 24s 94ms/step - loss: 1.0056 - acc: 0.6352\n",
      "Epoch 4/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.7478 - acc: 0.7235\n",
      "Epoch 5/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.4180 - acc: 0.8466\n",
      "Epoch 6/100\n",
      "256/256 [==============================] - 24s 94ms/step - loss: 0.2449 - acc: 0.9133\n",
      "Epoch 7/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.1399 - acc: 0.9531\n",
      "Epoch 8/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.1001 - acc: 0.9671\n",
      "Epoch 9/100\n",
      "256/256 [==============================] - 25s 96ms/step - loss: 0.0992 - acc: 0.9673\n",
      "Epoch 10/100\n",
      "256/256 [==============================] - 25s 96ms/step - loss: 0.0821 - acc: 0.9730\n",
      "Epoch 11/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0554 - acc: 0.9820\n",
      "Epoch 12/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0382 - acc: 0.9877\n",
      "Epoch 13/100\n",
      "256/256 [==============================] - 25s 96ms/step - loss: 0.0892 - acc: 0.9682\n",
      "Epoch 14/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0545 - acc: 0.9816\n",
      "Epoch 15/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0768 - acc: 0.9737\n",
      "Epoch 16/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0323 - acc: 0.9888\n",
      "Epoch 17/100\n",
      "256/256 [==============================] - 24s 96ms/step - loss: 0.0293 - acc: 0.9900\n",
      "Epoch 18/100\n",
      "256/256 [==============================] - 24s 96ms/step - loss: 0.0761 - acc: 0.9738\n",
      "Epoch 19/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0399 - acc: 0.9861\n",
      "Epoch 20/100\n",
      "256/256 [==============================] - 25s 96ms/step - loss: 0.0384 - acc: 0.9870\n",
      "Epoch 21/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0543 - acc: 0.9820\n",
      "Epoch 22/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0211 - acc: 0.9930\n",
      "Epoch 23/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0144 - acc: 0.9948\n",
      "Epoch 24/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0774 - acc: 0.9746\n",
      "Epoch 25/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0469 - acc: 0.9835\n",
      "Epoch 26/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0220 - acc: 0.9918\n",
      "Epoch 27/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0444 - acc: 0.9844\n",
      "Epoch 28/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0374 - acc: 0.9870\n",
      "Epoch 29/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0434 - acc: 0.9859\n",
      "Epoch 30/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0321 - acc: 0.9895\n",
      "Epoch 31/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0197 - acc: 0.9927\n",
      "Epoch 32/100\n",
      "256/256 [==============================] - 24s 96ms/step - loss: 0.0249 - acc: 0.9909\n",
      "Epoch 33/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0496 - acc: 0.9825\n",
      "Epoch 34/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0215 - acc: 0.9921\n",
      "Epoch 35/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0120 - acc: 0.9954\n",
      "Epoch 36/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0593 - acc: 0.9801\n",
      "Epoch 37/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0192 - acc: 0.9926\n",
      "Epoch 38/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0122 - acc: 0.9949\n",
      "Epoch 39/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0315 - acc: 0.9890\n",
      "Epoch 40/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0336 - acc: 0.9879\n",
      "Epoch 41/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0162 - acc: 0.9938\n",
      "Epoch 42/100\n",
      "256/256 [==============================] - 24s 94ms/step - loss: 0.0414 - acc: 0.9861\n",
      "Epoch 43/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0409 - acc: 0.9865\n",
      "Epoch 44/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0247 - acc: 0.9915\n",
      "Epoch 45/100\n",
      "256/256 [==============================] - 25s 96ms/step - loss: 0.0214 - acc: 0.9924\n",
      "Epoch 46/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0120 - acc: 0.9963\n",
      "Epoch 47/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0093 - acc: 0.9967\n",
      "Epoch 48/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0310 - acc: 0.9899\n",
      "Epoch 49/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0388 - acc: 0.9863\n",
      "Epoch 50/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0122 - acc: 0.9953\n",
      "Epoch 51/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0066 - acc: 0.9973\n",
      "Epoch 52/100\n",
      "256/256 [==============================] - 24s 94ms/step - loss: 0.0054 - acc: 0.9974\n",
      "Epoch 53/100\n",
      "256/256 [==============================] - 24s 94ms/step - loss: 0.0061 - acc: 0.9970\n",
      "Epoch 54/100\n",
      "256/256 [==============================] - 24s 94ms/step - loss: 0.0651 - acc: 0.9788\n",
      "Epoch 55/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0231 - acc: 0.9915\n",
      "Epoch 56/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0158 - acc: 0.9943\n",
      "Epoch 57/100\n",
      "256/256 [==============================] - 24s 94ms/step - loss: 0.0238 - acc: 0.9913\n",
      "Epoch 58/100\n",
      "256/256 [==============================] - 24s 94ms/step - loss: 0.0127 - acc: 0.9948\n",
      "Epoch 59/100\n",
      "256/256 [==============================] - 24s 94ms/step - loss: 0.0282 - acc: 0.9902\n",
      "Epoch 60/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0192 - acc: 0.9926\n",
      "Epoch 61/100\n",
      "256/256 [==============================] - 24s 94ms/step - loss: 0.0311 - acc: 0.9895\n",
      "Epoch 62/100\n",
      "256/256 [==============================] - 24s 94ms/step - loss: 0.0123 - acc: 0.9951\n",
      "Epoch 63/100\n",
      "256/256 [==============================] - 24s 94ms/step - loss: 0.0263 - acc: 0.9914\n",
      "Epoch 64/100\n",
      "256/256 [==============================] - 24s 93ms/step - loss: 0.0160 - acc: 0.9944\n",
      "Epoch 65/100\n",
      "256/256 [==============================] - 24s 93ms/step - loss: 0.0080 - acc: 0.9965\n",
      "Epoch 66/100\n",
      "256/256 [==============================] - 24s 94ms/step - loss: 0.0061 - acc: 0.9972\n",
      "Epoch 67/100\n",
      "256/256 [==============================] - 25s 97ms/step - loss: 0.0051 - acc: 0.9976\n",
      "Epoch 68/100\n",
      "256/256 [==============================] - 25s 96ms/step - loss: 0.0081 - acc: 0.9967\n",
      "Epoch 69/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0549 - acc: 0.9816\n",
      "Epoch 70/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0211 - acc: 0.9922\n",
      "Epoch 71/100\n",
      "256/256 [==============================] - 25s 96ms/step - loss: 0.0081 - acc: 0.9966\n",
      "Epoch 72/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0058 - acc: 0.9973\n",
      "Epoch 73/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0044 - acc: 0.9977\n",
      "Epoch 74/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0044 - acc: 0.9977\n",
      "Epoch 75/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0045 - acc: 0.9976\n",
      "Epoch 76/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0041 - acc: 0.9977\n",
      "Epoch 77/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0418 - acc: 0.9875\n",
      "Epoch 78/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0448 - acc: 0.9845\n",
      "Epoch 79/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0085 - acc: 0.9966\n",
      "Epoch 80/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0060 - acc: 0.9973\n",
      "Epoch 81/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0130 - acc: 0.9949\n",
      "Epoch 82/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0279 - acc: 0.9908\n",
      "Epoch 83/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0168 - acc: 0.9938\n",
      "Epoch 84/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0136 - acc: 0.9947\n",
      "Epoch 85/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0099 - acc: 0.9962\n",
      "Epoch 86/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0175 - acc: 0.9934\n",
      "Epoch 87/100\n",
      "256/256 [==============================] - 24s 94ms/step - loss: 0.0189 - acc: 0.9929\n",
      "Epoch 88/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0091 - acc: 0.9965\n",
      "Epoch 89/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0054 - acc: 0.9972\n",
      "Epoch 90/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0047 - acc: 0.9977\n",
      "Epoch 91/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0050 - acc: 0.9975\n",
      "Epoch 92/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0091 - acc: 0.9964\n",
      "Epoch 93/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0475 - acc: 0.9843\n",
      "Epoch 94/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0100 - acc: 0.9960\n",
      "Epoch 95/100\n",
      "256/256 [==============================] - 24s 94ms/step - loss: 0.0057 - acc: 0.9975\n",
      "Epoch 96/100\n",
      "256/256 [==============================] - 24s 96ms/step - loss: 0.0048 - acc: 0.9976\n",
      "Epoch 97/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0057 - acc: 0.9977\n",
      "Epoch 98/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0233 - acc: 0.9924\n",
      "Epoch 99/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0412 - acc: 0.9866\n",
      "Epoch 100/100\n",
      "256/256 [==============================] - 24s 95ms/step - loss: 0.0237 - acc: 0.9924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb887d976d8>"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#batch process, generating batches of tensor image data with real-time data augmentation\n",
    "gen = ImageDataGenerator()\n",
    "train_generator = gen.flow(x_train, y_train, batch_size=batch_size)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy'\n",
    "    , optimizer=keras.optimizers.Adam()\n",
    "    , metrics=['accuracy']\n",
    ")\n",
    "model.fit_generator(train_generator, steps_per_epoch=batch_size, epochs=epochs) #train for randomly selected one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ImageDataGenerator generates the batches of tensor data with real-time augmentation which will help to increase the image quality and help to learn better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "I0P8L-WVgq_f",
    "outputId": "363b3533-7701-46e9-a577-24fb5ccad52b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28709/28709 [==============================] - 5s 190us/step\n",
      "Train loss: 0.006535296773748992\n",
      "Train accuracy: 99.74224110905988\n",
      "3589/3589 [==============================] - 1s 178us/step\n",
      "Test loss: 3.2422732942846366\n",
      "Test accuracy: 57.23042630591277\n"
     ]
    }
   ],
   "source": [
    "#overall evaluation\n",
    "score = model.evaluate(x_train, y_train)\n",
    "print('Train loss:', score[0])\n",
    "print('Train accuracy:', 100*score[1])\n",
    "\n",
    "score = model.evaluate(x_test, y_test)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', 100*score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Evaluating the loss and accuracy for training and testing respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ndk4EIRQ7kt"
   },
   "outputs": [],
   "source": [
    "model.save('CNN_saved_final.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Saving the trained model here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pSQ34q0rTNka"
   },
   "outputs": [],
   "source": [
    "def detect_emotion(emotions):\n",
    "    \"\"\"\n",
    "    This function classifies the image by converting it NP into array and then predicting the results\n",
    "    \"\"\"\n",
    "    objects = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
    "    y_pos = np.arange(len(objects))\n",
    "    \n",
    "    plt.bar(y_pos, emotions, align='center', alpha=0.5)\n",
    "    plt.xticks(y_pos, objects)\n",
    "    plt.ylabel('percentage')\n",
    "    plt.title('emotion')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The above function classifies the input image by converting it into array and then predicts the results.\n",
    "* The np.arange(len(objects)) here returns evenly spaced value of length of objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 586
    },
    "colab_type": "code",
    "id": "OA7yg8NlTNkc",
    "outputId": "cb9d0b22-e25b-4ec0-ac29-0bc0c653b699"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py:98: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
      "  warnings.warn('grayscale is deprecated. Please use '\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEICAYAAACwDehOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = image.load_img(\"../motu.jpg\", grayscale=True, target_size=(48, 48))\n",
    "\n",
    "x = image.img_to_array(img) #Converting image to array\n",
    "x = np.expand_dims(x, axis = 0) #This will expand the shape of an array as we already converted the input image to 48x48\n",
    "\n",
    "x /= 255\n",
    "\n",
    "emotion_prediction = model.predict(x)\n",
    "detect_emotion(emotion_prediction[0])\n",
    "\n",
    "x = np.array(x, 'float32')\n",
    "x = x.reshape([48, 48]);\n",
    "\n",
    "plt.gray()\n",
    "plt.imshow(x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* img_to_array() - Converts the image to array\n",
    "* np.expand_dims() - Expands the shape of an array. Insert a new axis that will appear at the axis = 0 position in the expanded array shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenCV:\n",
    "\n",
    "OpenCV (Open Source Computer Vision) is a library of programming functions mainly aimed at real-time computer vision. The C++ API provides a class ‘videocapture’ for capturing video from cameras or for reading video files and image sequences. It is basically used to access the Webcam of our computer to capture real-time videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FjYhbZkZYZvz"
   },
   "outputs": [],
   "source": [
    "#Detecing expessions by feeding a video or webcam\n",
    "import cv2\n",
    "from collections import deque\n",
    "import operator\n",
    "emotion_queue = deque(maxlen=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s2gBUFBLYb2K"
   },
   "outputs": [],
   "source": [
    "def smooth_emotions(prediction):\n",
    "    \"\"\"\n",
    "    As the model will provide the mixture of results this function will give average of the emotions to 1 emotion\n",
    "    \n",
    "    \"\"\"\n",
    "    emotions = [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"]\n",
    "    emotion_values = {'Angry': 0.0, 'Disgust': 0.0, 'Fear': 0.0, 'Happy': 0.0, 'Sad': 0.0, 'Surprise': 0.0, 'Neutral': 0.0}\n",
    "\n",
    "    emotion_probability, emotion_index = max((val, idx) for (idx, val) in enumerate(prediction[0]))\n",
    "    emotion = emotions[emotion_index]\n",
    "\n",
    "    # Append the new emotion and if the max length is reached pop the oldest value out\n",
    "    emotion_queue.appendleft((emotion_probability, emotion))\n",
    "\n",
    "    # Iterate through each emotion in the queue and create an average of the emotions\n",
    "    for pair in emotion_queue:\n",
    "        emotion_values[pair[1]] += pair[0]\n",
    "\n",
    "    # Select the current emotion based on the one that has the highest value\n",
    "    average_emotion = max(emotion_values.items(), key=operator.itemgetter(1))[0]\n",
    "    return average_emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The above function takes processed image as input and provides the average of the emotions to 1 emotion from the mixture of results provided by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C8afZjQHYc3K"
   },
   "outputs": [],
   "source": [
    "# preprocessing the input image\n",
    "def process_image(roi_gray, img):\n",
    "        image_scaled = np.array(cv2.resize(roi_gray, (48, 48)), dtype=float)\n",
    "        image_processed = image_scaled.flatten()\n",
    "        image_processed = image_processed.reshape([-1, 48, 48, 1])\n",
    "\n",
    "        prediction = model.predict(image_processed)\n",
    "        emotion = smooth_emotions(prediction)\n",
    "\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        cv2.putText(img, \"Emotion: \" + emotion, (50, 450), font, 1, (0, 255, 255), 2)\n",
    "        cv2.imshow('img', img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The above function performs a series of preprocessing steps on input image by reshaping and flattening. Then it feeds the processed image to the model for it to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting objects through webcam\n",
    "\n",
    "The below code is used to detect objects through webcam. To capture a video, we need to create a VideoCapture object. Its argument can be either the device index or the name of a video file. Device index is just the number to specify which camera. Normally one camera will be connected (as in my case). So I simply pass 0 (or -1). You can select the second camera by passing 1 and so on. After that, you can capture frame-by-frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ykq-UDVNYewk"
   },
   "outputs": [],
   "source": [
    "# detecting human faces using HAAR cascade\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "#cap = cv2.VideoCapture(0) # 0 for webcam\n",
    "cap = cv2.VideoCapture(\"../face_detection.mp4\") # input the name of your video file here\n",
    "\n",
    "while True:\n",
    "    ret, img = cap.read()\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "        roi_gray = gray[y:y + h, x:x + w]\n",
    "        roi_color = img[y:y + h, x:x + w]\n",
    "        process_image(roi_gray, img)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e69CT-V-Ynbs"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "* After training this model for upto 200 epochs, we only got 85% training accuracy while the testing accuracy was around 50’s.\n",
    "\n",
    "* After looking at the predictions we came to conclusion that the model is overfitting the data. To improve the accuracy, we started tweaking the hyperparameters like Activation function, Cost functions, epochs, optimizers and every other parameter but accuracy was not affected much.\n",
    "\n",
    "* Initially, we trained this model for 200 epochs, but we found that the network reached plateau after almost 100 epochs. Since we did not want to overfit our model we trained it for 100 epochs.\n",
    "\n",
    "* Below table summarizes the values and the score that we could produce using the respective combination.\n",
    " \n",
    "\n",
    "\n",
    "Model No. | training accuracy | testing accuracy\n",
    "-----------------|----------------|----------------------\n",
    "CNN Sequential  | 99.7 %          | 57.2%       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nh78jObzYr0j"
   },
   "source": [
    "## Contribution\n",
    "\n",
    "In the above analysis:\n",
    "\n",
    "- 60% of the work is done by us which includes\n",
    "  * CNN implementation and accuracy improvement\n",
    "  * Live webcam code optimization\n",
    "\n",
    "- 40% of the work is taken from web, the links for which are cited below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w9IZMSi6Yv1k"
   },
   "source": [
    "## Citations\n",
    "\n",
    "* For face detection - https://www.youtube.com/watch?v=PmZ29Vta7Vc\n",
    "\n",
    "* For VGG19 and ResNet - https://github.com/tflearn/tflearn/tree/master/examples/images\n",
    "\n",
    "* For tflearn - http://tflearn.org/tutorials/\n",
    "\n",
    "* Article on Facial Emotion Recognition using CNN - http://sefiks.com/2018/01/01/facial-expression-recognition-with-keras/\n",
    "\n",
    "* VGG19 CNN - https://www.mathworks.com/help/deeplearning/ref/vgg19.html;jsessionid=ccf9599bd865b423281a56299a68"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UwZK78SXY0X7"
   },
   "source": [
    "## License\n",
    "\n",
    "<font size=\"4\">MIT License</font>\n",
    "    \n",
    "<b>Copyright (c) 2019 RUPESH ACHARYA, PREETAM JAIN</b>\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CNN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
